# Asthra LLM Quick Reference

## üöÄ Model Information

**HuggingFace Hub**: [`baijum/asthra-gemma3-lora`](https://huggingface.co/baijum/asthra-gemma3-lora)  
**Base Model**: `google/gemma-3-4b-it`  
**Method**: LoRA (Low-Rank Adaptation)  
**Size**: ~250MB (adapter only)  

## ‚ö° Quick Start Commands

### 1. Use from HuggingFace Hub
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("google/gemma-3-4b-it")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
model = PeftModel.from_pretrained(base_model, "baijum/asthra-gemma3-lora")

prompt = "Write a factorial function in Asthra:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200, temperature=0.3)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### 2. Start FastAPI Server
```bash
cd llmfinetuning
source .venv/bin/activate
python3 servers/fastapi_server.py --model ./outputs/asthra-gemma3-4b-v1 --port 8008
```

### 3. Use OpenAI-Compatible API
```python
import openai
client = openai.OpenAI(base_url='http://localhost:8008/v1', api_key='not-required')
response = client.chat.completions.create(
    model='asthra-gemma3-4b',
    messages=[{'role': 'user', 'content': 'Write a concurrent web server in Asthra'}],
    max_tokens=300
)
print(response.choices[0].message.content)
```

### 4. Use with Ollama
```bash
ollama create asthra -f Asthra-Modelfile
ollama run asthra "Write a factorial function in Asthra"
```

## üìÅ Key Files

| File | Purpose |
|------|---------|
| `outputs/asthra-gemma3-4b-v1/` | Trained LoRA adapter |
| `servers/fastapi_server.py` | Production API server |
| `Asthra-Modelfile` | Ollama configuration |
| `scripts/upload_to_huggingface.py` | HF Hub upload tool |
| `direct_model_test.py` | Direct model testing |
| `ASTHRA_LLM_DOCUMENTATION.md` | Complete documentation |

## üõ†Ô∏è API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Server health check |
| `/v1/models` | GET | List available models |
| `/v1/chat/completions` | POST | Generate code (OpenAI-compatible) |
| `/v1/completions` | POST | Text completion |

## üìä Performance

| Deployment | Speed | Use Case |
|------------|-------|----------|
| FastAPI Server | ~500 tokens/sec | Production API |
| Direct Model | ~300 tokens/sec | Custom integration |
| Ollama | ~200 tokens/sec | Local development |

## üîß Training Commands

```bash
# Original training command
python scripts/train_asthra_llm.py --config configs/gemma3_config.yaml

# Upload to HuggingFace
python3 scripts/upload_to_huggingface.py --model-path ./outputs/asthra-gemma3-4b-v1

# Test model directly
python3 direct_model_test.py
```

## üéØ Example Outputs

### Factorial Function
```asthra
fn factorial(n: u32) -> u64 {
    if n == 0 || n == 1 {
        return 1;
    }
    
    let mut result: u64 = 1;
    let start: u32 = n;
    
    repeat {
        result = result * (start as u64);
        start = start - 1;
    } while start > 0;
    
    return result;
}
```

### Concurrent Server
```asthra
fn start_server(port: u16) -> Result<void, String> {
    let (request_sender, request_receiver) = channel::<HttpRequest>();
    
    // Spawn worker tasks
    for i in 0..4 {
        let receiver = request_receiver.clone();
        spawn {
            handle_requests(receiver);
        };
    }
    
    let listener = TcpListener::bind(format!("0.0.0.0:{}", port))?;
    
    for connection in listener.incoming() {
        let request = parse_request(connection?)?;
        request_sender.send(request);
    }
    
    return Result.Ok(void);
}
```

## üîç Troubleshooting

### Common Issues

**Server won't start:**
```bash
# Check if port is in use
lsof -i :8008

# Check model files exist
ls -la outputs/asthra-gemma3-4b-v1/
```

**Memory issues:**
```python
# Use FP16 precision
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-4b-it",
    torch_dtype=torch.float16,
    device_map="auto"
)
```

**Import errors:**
```bash
# Install required packages
pip install transformers torch peft fastapi uvicorn openai
```

## üìö Documentation Links

- **Complete Guide**: [ASTHRA_LLM_DOCUMENTATION.md](./ASTHRA_LLM_DOCUMENTATION.md)
- **Project Summary**: [PROJECT_SUMMARY.md](./PROJECT_SUMMARY.md)
- **Ollama Setup**: [QUICK_OLLAMA_SETUP.md](./QUICK_OLLAMA_SETUP.md)
- **Server Guide**: [OPENAI_COMPATIBLE_SERVER_QUICKSTART.md](./OPENAI_COMPATIBLE_SERVER_QUICKSTART.md)

## üéâ Status

‚úÖ **Training**: Completed successfully  
‚úÖ **FastAPI Server**: Running and tested  
‚úÖ **HuggingFace Hub**: Model published  
‚úÖ **Ollama**: Integration working  
‚úÖ **Documentation**: Comprehensive guides available  

---

**üöÄ You now have the first production-ready Asthra Programming Language LLM!** 
