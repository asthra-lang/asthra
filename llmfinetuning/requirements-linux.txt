# Linux/RHEL Requirements for Asthra LLM Fine-Tuning
# Optimized for NVIDIA GPUs and Linux systems (RHEL, Ubuntu, CentOS)

# Core ML framework (Linux with CUDA support)
torch>=2.3.0+cu121  # CUDA 12.1 support
transformers>=4.46.0
datasets>=2.20.0
accelerate>=0.34.0
tokenizers>=0.20.0

# Mistral/Devstral specific
mistral-common>=1.5.5
tiktoken>=0.7.0

# Training framework with full Linux support
trl>=0.9.6
peft>=0.12.0
bitsandbytes>=0.43.0  # Quantization - Linux/CUDA only

# High-performance inference (Linux only)
vllm>=0.8.5  # High-performance serving
flash-attn>=2.5.0  # Flash attention for faster training

# Evaluation and monitoring
wandb>=0.17.0
tensorboard>=2.17.0
evaluate>=0.4.2

# Data processing
jsonlines>=4.0.0
tqdm>=4.66.1
numpy>=1.26.0
pandas>=2.2.0
pyyaml>=6.0.1
python-dotenv>=1.0.0

# Text processing and quality validation
nltk>=3.8.1
scikit-learn>=1.5.0
textstat>=0.7.3

# GGUF export for Ollama deployment (Linux optimized)
llama-cpp-python>=0.2.88  # Full support on Linux
gguf>=0.9.1

# Advanced optimization (Linux only)
deepspeed>=0.14.0  # Distributed training
xformers>=0.0.23  # Memory-efficient attention

# Hardware monitoring (NVIDIA GPUs)
nvidia-ml-py>=12.560.30  # NVIDIA GPU monitoring
gpustat>=1.1.1  # GPU status monitoring

# Development tools
pytest>=8.0.0
black>=24.0.0
isort>=5.13.0
flake8>=7.0.0
mypy>=1.10.0

# Jupyter notebooks and visualization
jupyter>=1.0.0
ipywidgets>=8.0.0
matplotlib>=3.9.0
seaborn>=0.13.0
plotly>=5.19.0

# Configuration management
hydra-core>=1.3.2
omegaconf>=2.3.0

# File handling and utilities
click>=8.1.7
rich>=13.7.0

# Memory optimization
safetensors>=0.4.3

# Model optimization (Linux optimized)
optimum>=1.21.0
onnx>=1.16.0  # Full support on Linux
onnxruntime-gpu>=1.18.0  # GPU acceleration

# System monitoring
psutil>=5.9.8
htop>=3.3.0  # System monitoring

# Additional Linux-specific optimizations
triton>=2.3.0  # GPU kernel compilation
einops>=0.8.0  # Tensor operations

# Distributed training support
mpi4py>=3.1.0  # MPI for distributed training (optional)

# Additional inference engines
ctranslate2>=4.0.0  # Fast inference engine

# RHEL/CentOS specific notes:
# - Install CUDA drivers: sudo yum install nvidia-driver cuda-toolkit
# - Install development tools: sudo yum groupinstall "Development Tools"
# - Install Python dev headers: sudo yum install python3-devel
# - For vLLM: Requires CUDA 11.8+ and Python 3.8+

# Performance profiling
py-spy>=0.3.14  # Python profiler
memory-profiler>=0.61.0  # Memory profiling

# Networking for distributed training
grpcio>=1.62.0
protobuf>=4.25.0 